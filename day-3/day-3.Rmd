---
output: html_notebook
editor_options: 
  chunk_output_type: console
---

# Day 3 - Loading Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

## (Almost) All You Need

```{r, eval=FALSE}
library(readr)
df <- read_csv(file = 'path/to/file')
```

A quick demo:

```{r}
df <- readr::read_csv('day-3/data/example.csv')
head(df)
```

## The `readr` Package

Load the `readr` package, which is part of the `tidyverse`.

```{r packages, message=FALSE}
library(readr)
```

To import your data, `readr` does three things:

1. Read your file and parse it into a matrix of strings
2. Determine the type of each column
3. Parse each column into its specific type

In order to parse a block of text into the rectangular format you want, `readr` 
needs to know how that text is delimited. Most times your delimiter will be a 
comma (i.e., csv - comma separated values). Other common delimiters are tab, 
pipe ('|'), and semicolon.

```{r}
read_delim('x,y,z\n1,2.5,apple', delim = ',')
```

```{r}
read_delim('x|y|z\n1|2.5|apple', delim = '|')
```

The `read_csv` and `read_tsv` are convenience wrappers around `read_delim`, 
with the `delim` argument pre-set for you.

```{r}
read_csv('x,y,z\n1,2,3')
```

```{r}
read_tsv('x\ty\tz\n1\t2\t3')
```

## Parsing Issues

Sometimes there will be an issue in your data file and `readr` will return a parsing failure. This is probably the most common problem you will have when 
reading in your data.

```{r}
df <- read_csv('day-3/data/parsing_failure.csv')
tail(df)
```

You can use the `problems` function to convert the output into a `tibble` as you work through the issues: 

```{r}
failures <- problems(df)
failures
```

Sometimes your file will return a lot of parsing failures, which can be 
overwhelming. A lot of times this is just the same error appearing in each row 
of a certain column. You can use the `unique` function to help break your 
issues down into more manageable chunks.

```{r}
unique(failures$col)
unique(failures$expected)
```


The issue is that the `read_*` functions will read in the first 1000 lines of a 
file and use that to guess the column type. The `guess_max` argument can be set 
higher if needed.

```{r}
df <- read_csv('day-3/data/parsing_failure.csv', guess_max = 1001)
tail(df)
```

Increasing `guess_max` can be time consuming, especially on larger files. A 
better practice is to explicity set the column types in the `col_types`
argument. In this case we tell `read_csv`  that `x` is a double vector and `y` 
is a character vector.

```{r}
df <- read_csv(
    'day-3/data/parsing_failure.csv',
    col_types = cols(
      x = col_double(), 
      y = col_character()
    )
  )

tail(df)
```

Setting your column types is a good practice to get in to. If your data file 
changes at some point, you'll quickly catch it in the parsing failures, as 
opposed to your code 'failing silently'. Setting the column types directly 
also improves the speed of the data read:

```{r, message=FALSE}
microbenchmark::microbenchmark(
  guess = read_csv(
    'day-3/data/example.csv'),
  specified = read_csv(
    'day-3/data/example.csv',
    col_types = cols(
      x = col_double(), 
      y = col_character()
    )
  ),
  control = list(warmup = 10)
)
```

Setting the type of each column can be time consuming on wide data sets. Use 
the `spec_*` family of functions to have `readr` create the initial guess 
of column specifications. You can then go back and edit the ones with 
parsing issues:

```{r}
columns <- spec_csv('day-3/data/parsing_failure.csv')
columns$cols$x <- col_double()

df <- read_csv('day-3/data/parsing_failure.csv', col_types = columns)
tail(df)
```


## Vector Types

There are four main data types within R:

1. logical
  - `c(TRUE, FALSE, FALSE)`
  - use `col_logical()` in `readr`

2. integer
  - `c(1L, 2L, 3L)`
  - use `col_integer()` in `readr`

3. double (sometimes called numeric)
  - `c(1.5, 2.5, 3.5)`
  - use `col_double()` in `readr`

4. character
  - `c('apple', 'banana', 'pear')`
  - use `col_character()` in `readr`

Properly setting vector types in `readr` can make a big difference on the amount
of memory your data consumes, especially for large files.

```{r}
zero_one <- c('0','1','1','0','0','0','1','1')
object.size(parse_integer(zero_one))
object.size(parse_double(zero_one))
object.size(parse_character(zero_one))
```

There are a couple other column specifications you can use within `readr`:

* `col_number()` can better handle numeric data that contains formatting
* `col_datetime()` for datetime data
  - Also `col_time()` for time data and `col_date` for date data

```{r}
parse_number(c("0%", "10%", "150%"))
parse_datetime("2010-10-01 21:45")
parse_date("1977-10-16")
```

The `col_datetime()` specification allows you to do some quick and easy 
comparisons:

```{r}
start <- parse_datetime("2010-10-01 21:45")
end <- parse_datetime("2010-10-08 21:45")
end - start
```

Be sure to check out the `readr` documentation to get a better understanding of 
the full range of date/time options.

## Saving Data

Once your data are loaded into R, use `saveRDS` to save your data frame as an 
`.rds` file. You can then use `readRDS` when you need to load it back into an 
R session. 

```{r}
saveRDS(df, file = 'day-3/data/example.rds')
readRDS('day-3/data/example.rds')
```

An `.rds` file will generally be smaller than the original `.csv` or 
`.txt`, and will often times load faster.

```{r}
file.size('day-3/data/example.csv')
file.size('day-3/data/example.rds')
```

```{r}
microbenchmark::microbenchmark(
  csv = read_csv(
    'day-3/data/example.csv',
    col_types = cols(
      x = col_double(), 
      y = col_character()
    )
  ),
  rds = readRDS('day-3/data/example.rds'),
  control = list(warmup = 10)
)
```

If you need to save your data back to `.csv`, you can use the `write_csv` 
function:

```{r}
write_csv(df, file = 'day-3/data/example_modified.csv')
```

## Your Turn

The `readr` package comes with a built-in `challenge.csv` file, which has been 
copied to `day-3/data/` for you. Reading it in with the base arguments 
will return 1,000 parsing errors. Try your best to work your way through the 
issues.

```{r}
df <- read_csv('day-3/data/challenge.csv')
```

## Excel Workbooks - `readxl`

For data in Excel (`.xlsx` / `.xls`) format, the `readxl` package provides 
similar functionality to `readr`:

```{r}
library(readxl)
df <- read_excel('day-3/data/example.xlsx')
head(df)
```

You can also specify the sheet and range of cells to use:

```{r}
df <- read_excel('day-3/data/example.xlsx', sheet = 'second_sheet', range = 'A1:A3')
head(df)
```

Note - For heavily formatted or othewise non-tidy data, you may want to try 
the `tidyxl` package.

## Importing from Other Statistical Software - `haven`

The `haven` package includes a number of functions to read and write data to 
and from other formats:

* `read_dta`/`write_dta` for Stata
* `read_sas`/`write_sas` for SAS
* `read_sav`/`write_sav` for SPSS

```{r}
library(haven)
df_stata <- read_dta('day-3/day-3/data/example.dta')
head(df_stata)
write_dta(df_stata, 'day-3/day-3/data/example.dta', version = 15)
```

## Non-tabular Data - `jsonlite`

```{r}
library(jsonlite)
data_json <- fromJSON('day-3/data/sample.json')
```

## Resources

* [r4ds.had.co.nz/data-import.html](r4ds.had.co.nz/data-import.html)
* [readr vignette](https://cran.r-project.org/web/packages/readr/vignettes/readr.html)
